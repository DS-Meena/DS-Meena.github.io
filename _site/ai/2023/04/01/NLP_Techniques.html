<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>NLP Techniques to get started with | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="NLP Techniques to get started with" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/ai/2023/04/01/NLP_Techniques.html" />
<meta property="og:url" content="http://localhost:4000/ai/2023/04/01/NLP_Techniques.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-01T15:08:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="NLP Techniques to get started with" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-04-01T15:08:10+05:30","datePublished":"2023-04-01T15:08:10+05:30","description":"Introduction","headline":"NLP Techniques to get started with","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2023/04/01/NLP_Techniques.html"},"url":"http://localhost:4000/ai/2023/04/01/NLP_Techniques.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">NLP Techniques to get started with</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-04-01T15:08:10+05:30" itemprop="datePublished">Apr 1, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>This blog provides an overview of Natural Language Processing (NLP) and describes various NLP techniques such as syntax, n-grams, tokenization, Markov models, text categorization, and semantics. These techniques can be used for information extraction, language identification, machine translation, and sentiment analysis. This blog also includes code snippets and examples to illustrate the implementation of these techniques. In this, we will be using basic python libraries to implement the NLP techniques.</p>

<h2 id="nlp">NLP</h2>

<p>Natural Language Processing (NLP) is a field of AI that focuses on the interaction between human language and computers. It involves developing algorithms and computational models that enable computers to process and analyze human language.</p>

<p>It has many applications like:</p>

<ul>
  <li>Automatic summarization</li>
  <li>Information extraction</li>
  <li>Language identification</li>
  <li>Machine translation</li>
  <li>Named entity recognition</li>
  <li>Speech recognition</li>
  <li>Text classification</li>
  <li>Word sense disambiguation</li>
</ul>

<h2 id="nlp-techniques">NLP Techniques</h2>

<p>NLP Techniques can be broadly classified into 2 categories, syntax based and semantics based.</p>

<h2 id="1-syntax">1. Syntax</h2>

<p>Syntax refers to the arrangement of words to make a sentence. It involves using formal grammar rules, such as context-free grammar, to generate sentences in a language.</p>

<h3 id="formal-grammar"><strong>Formal Grammar</strong></h3>

<p>Formal grammar is a set of rules that define the structure of a language. It is a system of rules for generating sentences in a language. These rules are used to specify how words and phrases can be combined to form sentences in the language.</p>

<p>Formal grammar can be used to describe both natural and artificial languages. Formal grammar is used to analyze the structure of natural languages and to model the structure of artificial languages like programming languages.</p>

<p>Formal grammar is often used to create programming languages, which are artificial languages used by computers to communicate with humans. These programming languages are designed to be easily understood by both humans and machines.</p>

<h3 id="context-free-grammar"><strong>Context-Free Grammar</strong></h3>

<p>A <strong>context-free grammar</strong> (CFG) is a type of formal grammar.</p>

<p>In a CFG, non-terminal symbols (e.g. N, V, D) represent parts of speech, while terminal symbols (e.g. “she,” “saw,” “the,” and “city”) represent actual words. Rules are then applied to generate sentences regardless of context, such as NP → N | D N, VP → V | V NP, and S → NP | VP. We can use the NLTK library to print the parse tree for a sentence.</p>

<p>Example:</p>

<p>Sentence → She saw the city.</p>

<p>Non-terminal symbols → N, V, D</p>

<p>Terminal Symbols → She, saw, the, city.</p>

<p>N (Noun) → She | City | car | Harry | ……</p>

<p>V (Verb) → Saw | ate | walked | ….</p>

<p>D (Determiner) → the | a | an | …..</p>

<p>P (Preposition) → to | on | over | ….</p>

<p>Adj (Adjective) → blue | busy | old | ….</p>

<p><strong>Rules</strong></p>

<ol>
  <li>NP → N | D N</li>
  <li>VP → V | V NP</li>
  <li>S → NP | VP</li>
</ol>

<p>Here’s an example code snippet using NLTK to generate a parse tree for the sentence “She saw the city.”:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nltk</span>

<span class="c1"># Create Grammar
</span><span class="n">grammar</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">CFG</span><span class="p">.</span><span class="nf">fromstring</span><span class="p">(</span><span class="sh">"""</span><span class="s">
    S -&gt; NP VP

    AP -&gt; A | A AP
    NP -&gt; N | D NP | AP NP | N PP
    PP -&gt; P NP
    VP -&gt; V | V NP | V NP PP

    A -&gt; </span><span class="sh">"</span><span class="s">big</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">small</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">dry</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">wide</span><span class="sh">"</span><span class="s">
    D -&gt; </span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">an</span><span class="sh">"</span><span class="s">
    N -&gt; </span><span class="sh">"</span><span class="s">she</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">city</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">car</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">street</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">dog</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">binoculars</span><span class="sh">"</span><span class="s">
    P -&gt; </span><span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">over</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">before</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">below</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">with</span><span class="sh">"</span><span class="s">
    V -&gt; </span><span class="sh">"</span><span class="s">saw</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="s">walked</span><span class="sh">"</span><span class="s">
</span><span class="sh">"""</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nc">ChartParser</span><span class="p">(</span><span class="n">grammar</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">Sentence: </span><span class="sh">"</span><span class="p">).</span><span class="nf">split</span><span class="p">()</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
        <span class="n">tree</span><span class="p">.</span><span class="nf">pretty_print</span><span class="p">()</span>
        <span class="n">tree</span><span class="p">.</span><span class="nf">draw</span><span class="p">()</span>
        <span class="c1"># break  # print only a single tree
</span><span class="k">except</span> <span class="nb">ValueError</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No parse tree possible.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Outputs will we like this</p>

<p><img src="/assets/2024/September/cfg1-output-768x538.png" alt="cfg1-output.png" /></p>

<p>Note that this parse tree shows the syntactic structure of the sentence, with non-terminal symbols representing parts of speech, and terminal symbols representing actual words.</p>

<h3 id="n-grams">N-Grams</h3>

<p>An N-gram is a contiguous sequence of N items from a sample of text. N-grams can be used to model the probability of the next word in a sentence, given the previous words.</p>

<ul>
  <li>Character N-gram: A contiguous sequence of N characters from a sample of text.</li>
  <li>Word N-gram: A contiguous sequence of N words from a sample of text.</li>
  <li>Unigram: A contiguous sequence of 1 item from a sample of text.</li>
  <li>Bigram: A contiguous sequence of 2 items from a sample of text.</li>
  <li>Trigram: A contiguous sequence of 3 items from a sample of text.</li>
</ul>

<p>Here’s an example Python code to generate N-grams:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
		<span class="sh">"""</span><span class="s">Calculate the term frequencies of N grams</span><span class="sh">"""</span>

    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">sys</span><span class="p">.</span><span class="nf">exit</span><span class="p">(</span><span class="sh">"</span><span class="s">Usage: python ngrams.py N corpus</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading data...</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

    <span class="c1"># Compute n-grams
</span>    <span class="n">ngrams</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">nltk</span><span class="p">.</span><span class="nf">ngrams</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>

    <span class="c1"># Print most common n-grams
</span>    <span class="k">for</span> <span class="n">ngram</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">ngram</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="n">contents</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Read all files and extract words
</span>    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">contents</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span>
                <span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span>
                <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
                <span class="k">if</span> <span class="nf">any</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">isalpha</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">word</span><span class="p">)</span>
            <span class="p">])</span>
    <span class="k">return</span> <span class="n">contents</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>

<p>The output will show the most common N-grams in the corpus.</p>

<p><img src="/assets/2024/September/word-n-gram-output.png" alt="word-n-gram-output.png" /></p>

<h3 id="tokenization">Tokenization</h3>

<p>Tokenization is the task of splitting a sequence of characters into pieces (tokens). <strong>Word tokenization</strong> refers to the task of splitting a sequence of characters into words, while <strong>sentence tokenization</strong> refers to the task of splitting a sequence of characters into sentences.</p>

<p>Here’s an example Python code using NLTK to perform word and sentence tokenization:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nltk</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="sh">"</span><span class="s">This is a sentence. This is another sentence.</span><span class="sh">"</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">sent_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

</code></pre></div></div>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', 'sentence', '.']
['This is a sentence.', 'This is another sentence.']
</code></pre></div></div>

<p>Tokenizing certain words can be difficult, such as words with contractions like “can’t” or hyphenated words like “well-cut”. In these cases, special processing may be required to accurately tokenize the words.</p>

<p>Example: “Whatever remains, however improbable, must be the truth.”</p>

<p>Tokenized: [“Whatever”, “remains,”, “however”, “improbable”, “must”, “be”, “the”, “truth.”]</p>

<p>Remove the commas and periods.</p>

<p>Word tokenized vector: [“Whatever”, “remains”, “however”, “improbable”, “must”, “be”, “the”, “truth”]</p>

<h3 id="markov-models">Markov Models</h3>

<p>Markov models are a type of probabilistic model that can be used to generate sequences of data. They are often used in natural language processing to generate text that appears to be similar to human-written text.</p>

<p>A Markov model is a simple model that takes the current state of a system and uses it to predict the next state. In the case of text generation, the system is the sequence of words in a sentence, and the state is the current word. The model uses the current word to predict the next word in the sequence.</p>

<p>Here’s a diagram illustrating the basic concept of Markov models:</p>

<p><img src="/assets/2024/September/image-1280x298.png" alt="Untitled" /></p>

<p>Here’s an example Python code to generate text using a Markov model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">markovify</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># Read text from file
</span><span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">sys</span><span class="p">.</span><span class="nf">exit</span><span class="p">(</span><span class="sh">"</span><span class="s">Usage: python generator.py sample.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="c1"># Train model
</span><span class="n">text_model</span> <span class="o">=</span> <span class="n">markovify</span><span class="p">.</span><span class="nc">Text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Generate sentences
</span><span class="nf">print</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">text_model</span><span class="p">.</span><span class="nf">make_sentence</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">()</span>
</code></pre></div></div>

<p>This code uses the markovify library to train a Markov model on a sample of text from a file. It then generates five sentences using the trained model.</p>

<p><img src="/assets/2024/September/Untitled-14.png" alt="Untitled" /></p>

<h3 id="text-categorization">Text Categorization</h3>

<p>Text categorization refers to the task of assigning predefined categories or labels to a set of documents based on their content.  Text categorization has various applications and can be used in different cases, such as:</p>

<ul>
  <li>Spam vs. not spam</li>
  <li>Happy vs. sad (sentiment)</li>
  <li>Classify documents into different topics.</li>
</ul>

<p>Examples:</p>

<p>🙂 My grandson loved it! So much fun.</p>

<p>☹️ The product broke after a few days.</p>

<p>🙂 It’s one of the best games I’ve played in a long time.</p>

<p>☹️ It’s kind of cheap and flimsy, not worth it.</p>

<h3 id="text-categorization-methods">Text Categorization Methods</h3>

<p>Text categorization methods include the bag of words model, topic modeling, and term frequency-inverse document frequency (TF-IDF). The <strong>bag of words model</strong> represents text as an unordered collection of words, while <strong>topic modeling</strong> involves discovering the underlying topics in a set of documents. <strong>TF-IDF</strong> ranks the importance of words in a document based on their frequency and rarity across the entire corpus. These methods can be used to classify documents into different categories or labels based on their content, such as sentiment analysis or spam detection.</p>

<p>Here, we will talk about Bag of words model for text categorization.</p>

<h3 id="1-bag-of-words-model">1. Bag of Words Model</h3>

<p>The <strong>bag of words model</strong> represents text as an unordered collection of words. In this model, a text is represented as a bag (multiset) of its words, disregarding grammar and word order.</p>

<p>This model can be used for various tasks such as sentiment analysis.</p>

<p>In the bag of words model, a text document is represented as a sparse vector (entries represent the frequency of word) of word occurrences, and the <strong>Naive Bayes classifier</strong> can use this representation to classify the document into one of several pre-defined categories.</p>

<p><strong>Bayes’ Rule (Naive Bayes Classifier)</strong></p>

<p>Naive Bayes classifier is a machine learning algorithm based on Baye’s rule. It can be used for text classification, and it can be applied to a bag of words model.</p>

<p>It is called “naive” because it assumes that all the features are independent of each other, which is not always the case. Despite this, the Naive Bayes Classifier has been shown to perform well in various applications.</p>

<p>The rule is based on conditional probability, where the probability of an event happening depends on the occurrence of another event.</p>

<p>The formula for conditional probability is given by</p>

<p>$P(b\lvert a) = \frac {P(b, a)}{P(a)}$, where $a$ and $b$ are two events.</p>

<p>On the other hand, the formula for conditional probability with a condition is</p>

<p>$P(b \lvert a) = \frac {P(b)P(a \lvert b)}{P(a)}$, where $P(a\lvert b)$ is the probability of occurrence of event $a$ given that $b$ has occurred.</p>

<p>In terms of <strong>univariate distribution</strong>, we can use a table to show the probabilities of two events occurring. For example, we can use the following table to show the probabilities of a happy and a sad face:</p>

<table>
  <thead>
    <tr>
      <th>🙂</th>
      <th>☹️</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.49</td>
      <td>0.51</td>
    </tr>
  </tbody>
</table>

<p>On the other hand, for <strong>bivariate distribution</strong>, we can use a table to show the probabilities of one event given second event. For example, we can use the following table to show the probabilities of different words given happy or sad face:</p>

<table>
  <thead>
    <tr>
      <th>P(word</th>
      <th>emotioin)</th>
      <th>🙂</th>
      <th>☹️</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>my</td>
      <td>0.30</td>
      <td>0.20</td>
      <td> </td>
    </tr>
    <tr>
      <td>grandson</td>
      <td>0.01</td>
      <td>0.02</td>
      <td> </td>
    </tr>
    <tr>
      <td>loved</td>
      <td>0.32</td>
      <td>0.08</td>
      <td> </td>
    </tr>
    <tr>
      <td>it</td>
      <td>0.30</td>
      <td>0.40</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>It is important to note that these tables are just examples and can be used for various purposes. By using Bayes’ Rule and such tables, we can gain valuable insights into various phenomena and make informed decisions.</p>

<p>Here’s an example Python code for implementing a Naive Bayes Classifier for sentiment analysis:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="c1"># Read data from files
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">sys</span><span class="p">.</span><span class="nf">exit</span><span class="p">(</span><span class="sh">"</span><span class="s">Usage: python sentiment.py corpus</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">positives</span><span class="p">,</span> <span class="n">negatives</span> <span class="o">=</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Create a set of all words
</span>    <span class="n">words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">positives</span><span class="p">:</span>
        <span class="n">words</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">negatives</span><span class="p">:</span>
        <span class="n">words</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>

    <span class="c1"># Extract features from text
</span>    <span class="n">training</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">training</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">generate_features</span><span class="p">(</span><span class="n">positives</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="sh">"</span><span class="s">Positive</span><span class="sh">"</span><span class="p">))</span>
    <span class="n">training</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">generate_features</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="sh">"</span><span class="s">Negative</span><span class="sh">"</span><span class="p">))</span>

    <span class="c1"># Classify a new sample
</span>    <span class="n">classifier</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">NaiveBayesClassifier</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">s: </span><span class="sh">"</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="nf">classify</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">words</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">result</span><span class="p">.</span><span class="nf">samples</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="nf">prob</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">extract_words</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">set</span><span class="p">(</span>
        <span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">any</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">isalpha</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">word</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">positives.txt</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">negatives.txt</span><span class="sh">"</span><span class="p">]:</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span>
                <span class="nf">extract_words</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">().</span><span class="nf">splitlines</span><span class="p">()</span>
            <span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">generate_features</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
        <span class="n">features</span><span class="p">.</span><span class="nf">append</span><span class="p">(({</span>
            <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span>
        <span class="p">},</span> <span class="n">label</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">features</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">document</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="n">document_words</span> <span class="o">=</span> <span class="nf">extract_words</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">document_words</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">prob_classify</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>

<p>This code uses the NLTK library to implement a Naive Bayes Classifier for sentiment analysis. It first loads the data from two text files (“positives.txt” and “negatives.txt”), then extracts the words and generates features for each document. The classifier is then trained on these features, and the user can input a new sentence to be classified as either “positive” or “negative”. The output includes the probability of the sentence belonging to each class.</p>

<p>Output:</p>

<p><img src="/assets/2024/September/Untitled-15.png" alt="Untitled" /></p>

<h3 id="2-topic-modelling"><strong>2. Topic modelling</strong></h3>

<p><strong>Topic modelling</strong> is a technique used to discover underlying topics in a collection of text documents. It involves using algorithms to identify patterns and group similar words together, resulting in a set of topics that represent the main themes within the text. Topic modelling is often used in text mining and information retrieval applications, such as search engines and recommendation systems. Some popular topic modelling algorithms include Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF).</p>

<h3 id="3-tf-idf">3. TF-IDF</h3>

<p>TF-IDF (term frequency-inverse document frequency) is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is calculated by multiplying the frequency of a word in a document by the inverse frequency of the word in the corpus.</p>

<p>This measure is useful for text classification, where it can be used to identify important words or features in a document that can be used to classify it into one or more categories.</p>

<p>Here’s an example Python code to calculate TF-IDF scores:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Example documents
</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">This is the first document.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">This is the second document.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">And this is the third one.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Is this the first document?</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Calculate TF-IDF scores
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">()</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># Print feature names and scores
</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Document </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">tfidf</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">toarray</span><span class="p">()).</span><span class="nf">flatten</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">feature_names</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">tfidf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<p>This code uses the TfidfVectorizer class from the scikit-learn library to calculate TF-IDF scores for a set of example documents. The output includes the feature names and scores for each document.</p>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Document 0:
  the: 0.4694
  this: 0.3793
  is: 0.3793
  first: 0.3310
  document: 0.3310

Document 1:
  the: 0.4694
  this: 0.3793
  is: 0.3793
  second: 0.3310
  document: 0.3310

Document 2:
  the: 0.4694
  this: 0.3030
  is: 0.3030
  third: 0.4178
  and: 0.4178
  one: 0.4178

Document 3:
  the: 0.4694
  this: 0.3793
  is: 0.3793
  first: 0.3310
  document: 0.3310

</code></pre></div></div>

<p>In this example, the TF-IDF scores are calculated for the words in each document, and the most important words are identified based on their scores. These scores can be used as features for text classification algorithms, such as Naive Bayes or Support Vector Machines (SVMs).</p>

<h3 id="information-retrieval"><strong>Information retrieval</strong></h3>

<p>Information retrieval is the task of finding relevant information in a collection of documents. This is often done using search engines, which use various techniques to index and search through large volumes of text. Information retrieval techniques include keyword-based search, TF-IDF, and machine learning algorithms.</p>

<h3 id="tf-idf">TF-IDF</h3>

<p>TF-IDF (term frequency-inverse document frequency) is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is calculated by multiplying the <strong>term frequency</strong> (number of occurrences of a word in a document) by the <strong>inverse document frequency</strong> (logarithm of the total number of documents divided by the number of documents containing the word). TF-IDF is commonly used for text classification, information retrieval, and other applications where the importance of words needs to be evaluated.</p>

<p>Inverse document frequency is a measure of how common or rare a word is across document.</p>

<p>$IDF =  log_e(\frac {Total-Documents}{Num-Documents-containing-word })$</p>

<p>$TF-IDF = TF * IDF$</p>

<p>TF-IDF is commonly used in search engines to rank documents based on their relevance to a query. When a user searches for a query, the search engine uses TF-IDF to calculate the relevance of each document in the corpus and returns the most relevant documents to the user. The documents with the highest TF-IDF score for a given query are considered the most relevant to that query. Therefore, TF-IDF is a useful tool for information retrieval, as it allows users to quickly and easily find relevant documents in a large corpus.</p>

<p>Following repo has query search code, this uses the same TF-IDF technique: 
<a href="https://github.com/DS-Meena/Query-Retrieval-using-NLP">Query Retrieval using NLP</a></p>

<h2 id="2-semantics">2. Semantics</h2>

<p>Semantics-based NLP techniques involve understanding the meaning and context of words and phrases in a language. Some techniques include:</p>

<ul>
  <li><strong>Semantic Role Labeling</strong>: Identifying the semantic roles (such as agent, patient, and location) of words in a sentence.</li>
  <li><strong>Word Sense Disambiguation</strong>: Identifying the correct meaning of a word based on the context in which it is used.</li>
  <li><strong>Named Entity Recognition</strong>: Identifying and classifying named entities (such as people, organizations, and locations) in a text.</li>
  <li><strong>Sentiment Analysis</strong>: Determining the emotional tone or attitude expressed in a text.</li>
  <li><strong>Topic Modeling</strong>: Analyzing the words and themes in a corpus of documents to identify topics and patterns.</li>
  <li><strong>Relation Extraction</strong>: Identifying and extracting relationships between entities in a text.</li>
</ul>

<p>These techniques are often used in combination with syntax-based techniques to gain a deeper understanding of language.</p>

<h3 id="information-extraction">Information extraction</h3>

<p>Information extraction is a subfield of natural language processing that involves automatically extracting structured information from unstructured or semi-structured documents. This can include identifying and extracting relevant facts, entities, and relationships from text. Some techniques used in information extraction include <strong>named entity recognition</strong>, <strong>relation extraction</strong>, and <strong>text classification</strong>. Information extraction is often used in applications such as search engines, recommender systems, and chatbots to extract relevant information and provide more personalized responses to users.</p>

<p>An example of information extraction would be extracting the names and locations mentioned in a set of news articles. <strong>Named entity recognition</strong> could be used to identify the names of people, organizations, and locations mentioned in the text, while <strong>relation extraction</strong> could be used to identify the relationships between these entities (such as “Barack Obama is the President of the United States”). This information could then be used to create a database of news articles organized by the people, organizations, and locations mentioned in each article, making it easier to search and analyze the content.</p>

<p>Example →</p>

<p>facebook.txt</p>

<p>..”When Facebook was founded in 2004”.</p>

<p>Amazon.txt</p>

<p>..”When Amazon was founded in 1994”.</p>

<p>When {company} was founded in {year}.</p>

<h3 id="word-net">Word Net</h3>

<p>WordNet is a lexical database for the English language that groups words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between these synonym sets. WordNet can be used for various natural language processing tasks, such as word sense disambiguation, information retrieval, and text classification.</p>

<p>WordNet is organized hierarchically, with each synset representing a different concept or sense of a word. Synsets are linked together by various semantic relations, such as <strong>hypernymy</strong> (a word that is a more general or abstract form of another word) and <strong>hyponymy</strong> (a word that is a more specific or concrete form of another word).</p>

<p><img src="/assets/2024/September/image-147.png" alt="Fig: Hierarchy of [Wordnet]" /></p>

<p>*Fig: Hierarchy of <a href="https://analyticsindiamag.com/a-complete-guide-to-using-wordnet-in-nlp-applications/">Wordnet</a> *</p>

<p>Here is an example Python code to access WordNet using the NLTK library:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># Find synsets for the word "dog"
</span><span class="n">synsets</span> <span class="o">=</span> <span class="n">wn</span><span class="p">.</span><span class="nf">synsets</span><span class="p">(</span><span class="sh">"</span><span class="s">dog</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the definitions and examples for each synset
</span><span class="k">for</span> <span class="n">synset</span> <span class="ow">in</span> <span class="n">synsets</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">synset</span><span class="p">.</span><span class="nf">name</span><span class="p">()</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">synset</span><span class="p">.</span><span class="nf">definition</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">synset</span><span class="p">.</span><span class="nf">examples</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s"> - </span><span class="si">{</span><span class="n">example</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Find hypernyms for the first synset
</span><span class="n">hypernyms</span> <span class="o">=</span> <span class="n">synsets</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">hypernyms</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Hypernyms: </span><span class="si">{</span><span class="p">[</span><span class="n">h</span><span class="p">.</span><span class="nf">name</span><span class="p">()</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hypernyms</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Find hyponyms for the first synset
</span><span class="n">hyponyms</span> <span class="o">=</span> <span class="n">synsets</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">hyponyms</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Hyponyms: </span><span class="si">{</span><span class="p">[</span><span class="n">h</span><span class="p">.</span><span class="nf">name</span><span class="p">()</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hyponyms</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<p>This code uses the NLTK library to access WordNet and find the synsets for the word “dog”. It then prints the definitions and examples for each synset, as well as the hypernyms and hyponyms for the first synset.</p>

<p>Output:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dog.n.01: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds
<span class="p"> -</span> the dog barked all night
canis_familiaris.n.01: a domesticated mammal; a familiar spirit kept by a witch or wizard
<span class="p"> -</span> the dog is a familiar of the witch
frump.n.01: a dull unattractive unpleasant girl or woman
<span class="p"> -</span> she got a reputation as a frump
cad.n.01: someone who is morally reprehensible
<span class="p"> -</span> you dirty dog
<span class="p"> -</span> they treated him like a cur
<span class="p"> -</span> the lowest cur listened to his order
<span class="p"> -</span> get out of my sight, you wretch
<span class="p"> -</span> what a dirty dog
Hypernyms: ['canine.n.02', 'domestic_animal.n.01']
Hyponyms: ['Airedale.n.01', 'Bouvier_des_Flandres.n.01', 'Canis_dingo.n.01', 'Canis_familiaris.n.01', 'Canis_minor.n.01', 'Doberman.n.01', 'Great_Pyrenees.n.01', 'Labrador_retriever.n.01', 'Leonberg.n.01', 'Mexican_hairless.n.01', 'Newfoundland.n.01', 'Pekingese.n.01', 'Pomeranian.n.01', 'Poodle.n.01', 'pug.n.01', 'puppy.n.01', 'toy_dog.n.01']

</code></pre></div></div>

<p>In this example, the synsets for the word “dog” are found and printed, along with their definitions and examples. The hypernyms and hyponyms for the first synset are also printed.</p>

<h3 id="word-representation">Word Representation</h3>

<p>Word representation is the process of representing words in a language as numerical vectors that can be used as input to machine learning models. This is necessary because most machine learning algorithms can only work with numerical data, not text data. There are many techniques for word representation, some of the most popular include:</p>

<ul>
  <li><strong>One-hot encoding</strong>: A simple technique where each word is represented as a binary vector, with a 1 in the position corresponding to the word’s index in the vocabulary, and 0s elsewhere. This technique is often used as a baseline for comparison with more advanced techniques.</li>
  <li><strong>Word embeddings</strong>: A family of techniques where each word is represented as a dense vector in a high-dimensional space, with the vector’s coordinates encoding the word’s semantic and syntactic properties. Word embeddings are often learned using neural network models, such as Word2Vec or GloVe.</li>
  <li><strong>Distribution encoding</strong>: Distributional representation typically involves representing words as sparse vectors based on their co-occurrence with other words in a corpus. Word embeddings can be generated using distribution representation.</li>
  <li><strong>Contextualized embeddings</strong>: A recent development in word representation, where each word is represented as a vector that depends on the context in which it appears. Examples of contextualized embeddings include ELMo and BERT.</li>
</ul>

<h3 id="one-hot-representation"><strong>One-hot Representation</strong></h3>

<p>Example: “He wrote a book.”</p>

<p>he: [1, 0, 0, 0, …]</p>

<p>wrote: [0, 1, 0, 0, 0, …]</p>

<p>a: [0, 0, 1, 0, 0, …]</p>

<p>book: [0, 0, 0, 1, 0, 0, 0, …]</p>

<p>Words with similar meanings have similar indices.</p>

<p>For example, “wrote” and “authored” have similar vectors: [0, 1, 0, 0, 0, 0, 0] and [0, 0, 0, 1, 0, 0, 0], respectively.</p>

<p>Similarly, “book” and “novel” have similar vectors: [0, 0, 0, 1, 0, 0, 0, …] and [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], respectively.</p>

<p>The more similar the meanings of two words, the closer their vectors will be.</p>

<h3 id="distribution-representation"><strong>Distribution Representation</strong></h3>

<p>Here is an example of distribution representation:</p>

<p>“he wrote a book”</p>

<p>he           [-0.34, -0.03, 0.02, -0.18, 0.22, ….]</p>

<p>wrote    [-0.27, 0.40, 0.00, -0.65,  -0.15, ….]</p>

<p>a              [-0.12, -0.25, 0.29, -0.09, 0.40, ….]</p>

<p>book      [-0.23, -0.16, -0.05, -0.57, …….]</p>

<p>Each value represents a specific meaning, with similar meanings having similar values.</p>

<h3 id="word-embeddings"><strong>Word Embeddings</strong></h3>

<p>Word embeddings are dense vector representation of words in a high-dimensional space, where words with similar meanings are located closer together. [means less dimensions are required compared to one-hot representation]</p>

<p>Word2Vec is a neural network architecture that learns these word vectors, known as word embeddings.</p>

<p><strong>Train model to capture Relationship between words</strong></p>

<p>There are many existing mathematical techniques for capturing the important structure of a high-dimensional space in a low dimensional space.</p>

<p>For example, <a href="https://wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> (PCA) has been used to create word embeddings. Given a set of instances like bag of words vectors, PCA tries to find highly correlated dimensions that can be collapsed into a single dimension.</p>

<p>“Word2vec is used to learn word embeddings” is a more commonly used phrasing in the NLP community, as it emphasizes the fact that word embeddings are learned from data rather than generated from scratch.</p>

<p>Here’s an example Python code to generate word vectors using Word2Vec:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="c1"># Example sentences
</span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">I love to play football</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">She enjoys playing chess</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">He hates swimming</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">They like to dance</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">We prefer reading books</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">You dislike running</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Preprocess sentences (remove punctuation and lowercase)
</span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">).</span><span class="nf">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="c1"># Word2Vec model learn the word embeddings
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Print word vectors for some example words
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Word vector for </span><span class="sh">'</span><span class="s">football</span><span class="sh">'</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">"</span><span class="s">football</span><span class="sh">"</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Word vector for </span><span class="sh">'</span><span class="s">reading</span><span class="sh">'</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">"</span><span class="s">reading</span><span class="sh">"</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Word vector for </span><span class="sh">'</span><span class="s">swimming</span><span class="sh">'</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">"</span><span class="s">swimming</span><span class="sh">"</span><span class="p">])</span>

</code></pre></div></div>

<p>In the above below, the Word2Vec model is learning word embeddings for the specific sentences provided. The Word2Vec model learns these embeddings by predicting the context of each word in the sentence, based on the words that surround it. By doing so, the model learns to represent words that have similar contexts or meanings, with similar vector representations in the embedding space.</p>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Word vector for 'football': [ 0.00961521 -0.01828998 -0.00957083  0.00582886 -0.01004235  0.00128278
  0.00485666 -0.00454647 -0.00431244 -0.00078377 -0.00065795 -0.01995155
  ...
Word vector for 'reading': [ 0.00572616 -0.00882261 -0.00071661  0.01290313 -0.01122897 -0.00116539
 -0.00823871 -0.00643388 -0.0110871  -0.00170786 -0.0056673  -0.01761658
  ...
Word vector for 'swimming': [ 0.01692077  0.00346931  0.00369379 -0.00355725  0.00599522 -0.01022142
 -0.00196455 -0.0164674   0.00604031 -0.00220976 -0.01128828  0.00769941
  ...

</code></pre></div></div>

<p>In this example, word vectors are generated for the words in the example sentences using the Word2Vec model. The word vectors are 100-dimensional, and some example vectors are printed for the words “football”, “reading”, and “swimming”.</p>

<h3 id="skip-gram-architecture">Skip-gram architecture</h3>

<p>The <strong>skip-gram architecture</strong> is a neural network model used for generating word embeddings. It is similar to the Word2Vec model, but instead of predicting a target word given a context window, it predicts the context window given a target word. This is useful for generating high-quality embeddings for rare or infrequent words, which may not appear frequently enough in a corpus to be accurately predicted using a context window.</p>

<p><img src="/assets/2024/September/1_SR6l59udY05_bUICAjb6-w.png" alt="Figure: Skip-gram architecture" /></p>

<p><em>Figure: Skip-gram architecture</em></p>

<p>Ex → Given dinner might generate breakfast, lunch, etc. Given book might generate memoir, novel, etc.</p>

<p>I hope you learned something useful in this blog ❤️❤️.</p>

  </div><a class="u-url" href="/ai/2023/04/01/NLP_Techniques.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
