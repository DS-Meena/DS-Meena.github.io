<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformer of AI World | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Transformer of AI World" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction ğŸš€" />
<meta property="og:description" content="Introduction ğŸš€" />
<link rel="canonical" href="http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html" />
<meta property="og:url" content="http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-06-23T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transformer of AI World" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-06-23T10:00:10+05:30","datePublished":"2023-06-23T10:00:10+05:30","description":"Introduction ğŸš€","headline":"Transformer of AI World","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html"},"url":"http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformer of AI World</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-06-23T10:00:10+05:30" itemprop="datePublished">Jun 23, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction-">Introduction ğŸš€</h2>

<p>Are you interested in natural language processing? ğŸ¤” If so, you might have heard of the Transformer, a neural network architecture that has achieved state-of-the-art results in several NLP tasks ğŸ†, such as machine translation ğŸŒ, text classification ğŸ“Š, and language modeling ğŸ“š. In this blog, weâ€™ll explore what the Transformer is, how it works, its advantages and disadvantages, and some of its applications. ğŸ§</p>

<h2 id="definition-">Definition ğŸ“–</h2>

<p>The Transformer is a type of neural network architecture that relies heavily on attention mechanisms ğŸ‘€. It was introduced in 2017 by Vaswani et al. and has since become one of the most popular neural network architectures in the field of natural language processing. ğŸŒŸ</p>

<p>Its success is largely due to its ability to leverage attention mechanisms, which enable the model to focus on specific parts of the input sequence ğŸ”. This allows the Transformer to process long sequences of text more efficiently than traditional recurrent neural networks. ğŸš€</p>

<h1 id="working-ï¸">Working âš™ï¸</h1>

<p>The Transformer architecture is a powerful deep learning model used for natural language processing. It is composed of two main parts: an encoder ğŸ”’ and a decoder ğŸ”“.</p>

<p>The encoder, which is the first part of the model, processes the input sequence by encoding its information into a set of vectors ğŸ“Š. These vectors capture the meaning of the input sequence and are then passed onto the decoder. ğŸ”„</p>

<p>The decoder is responsible for generating the output sequence, such as a translation or a summary ğŸ“. It does this by decoding the vectors produced by the encoder and generating a new sequence of words based on the encoded information. ğŸ¨</p>

<p><img src="/assets/2024/September/transformers.png" alt="Fig: Transformer - model architecture [[Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf)]" /></p>

<p><em>Fig: Transformer - model architecture [<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. (2017)</a>]</em></p>

<h2 id="encoder-">Encoder ğŸ”’</h2>

<p>The Encoder is made up of N=6 identical layers, each of which has 2 sub-layers. ğŸ§±ğŸ”„</p>

<ol>
  <li><strong>Multi-Head Self-Attention Mechanism ğŸ§ ğŸ‘€</strong>: This mechanism calculates multiple sets of attention weights using self-attention, with each set focusing on a different part of the input sequence.</li>
  <li>
    <p><strong>Feed Forward Network ğŸ”„â¡ï¸</strong>: This is a fully connected feed-forward network that is applied to each position separately and identically. For this, two linear transformations are used with ReLU activation between them:</p>

    <p>$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$</p>

    <p>Here, $x$ is the input vector, $W_1$ and $W_2$ are weight matrices, $b_1$ and $b_2$ are bias vectors, and $\max(0, x)$ is the ReLU activation function. ğŸ§®ğŸ“Š</p>
  </li>
</ol>

<h2 id="decoder-">Decoder ğŸ”“</h2>

<p>The decoder is also composed of N=6 identical layers. However, the decoder adds an extra layer: ğŸ”“ğŸ”¢</p>

<ol>
  <li><strong>Multi-Head Attention ğŸ­ğŸ‘¥</strong>: This layer does not use self-attention, but instead calculates attention weights over the output of the encoder stack. Its keys and values come from the output of the encoder layer, while its queries come from the previous decoder layer.</li>
</ol>

<p>After each sub-layer, normalization is performed using layer normalization ğŸ“. This process helps maintain stable activations throughout the network. ğŸ§ </p>

<h4 id="normalization">Normalization</h4>

<p>Normalization involves scaling the input features to have zero mean and unit variance, which helps reduce the effect of differences in the scale of the input features. ğŸ“ŠğŸ”</p>

<p>Layer normalization is defined as: ğŸ§®</p>

<p>$LayerNorm(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$</p>

<p>where $x$ is the input vector, $\mu$ and $\sigma$ are the mean and standard deviation of $x$, respectively, $\epsilon$ is a small constant to prevent division by zero, $\odot$ is element-wise multiplication, and $\gamma$ and $\beta$ are learnable scale and shift parameters, respectively. ğŸ”¬</p>

<p>In addition, positional encoding is added to the input embeddings of the encoder and decoder stacks. ğŸ¯ğŸ”¢</p>

<h4 id="positional-encoding">Positional encoding</h4>

<p>The positional encoding is defined as: ğŸ§˜ğŸ½</p>

<p>$PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})$</p>

<p>$PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p>

<p>where $pos$ is the position of the token in the sequence, $i$ is the dimension of the embedding vector, and $d_{model}$ is the total number of dimensions. ğŸ“</p>

<p>These layers allow the model to learn complex patterns and relationships within the input and output sequences. ğŸ•¸ï¸ The self-attention layers help the model focus on the most important parts of the input sequence ğŸ”, while the feed-forward layers help it make predictions based on the encoded information. ğŸ¯</p>

<p>Think in context of training, both input and output are provided. During backpropagation, the weight matrices of the scaled dot-product attention are updated, as well as the parameters of the feedforward neural layers. This process aims to improve the accuracy of the next output and make the attention weights more reasonable. ğŸ”„ğŸ“ˆ</p>

<p>Overall, the Transformer architecture is a highly effective model for a wide range of natural language processing tasks, including machine translation ğŸŒ and text summarization ğŸ“. By using multiple layers of self-attention and feed-forward neural networks, it is able to capture complex relationships and patterns within the input and output sequences, making it a powerful tool for language understanding and generation. ğŸš€ğŸ—£ï¸</p>

<p>Letâ€™s dive into the attention mechanism used in transformers! ğŸ¤¿ğŸ§ </p>

<h2 id="multi-head-attention-">Multi-Head Attention ğŸ­ğŸ”</h2>

<p>The multi-head attention operation is like a symphony of smaller attention operations ğŸ»ğŸº. These mini-operations work together in parallel, creating a harmonious final output ğŸµ.</p>

<p>Hereâ€™s the magical equation for multi-head attention ğŸ§™â€â™‚ï¸:</p>

<p>$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, â€¦, \text{head}_h)W^O$</p>

<p>Each smaller operation is called an attention â€œheadâ€ ğŸ‘¤. The output of the $i$-th head is calculated like this ğŸ§®:</p>

<p>$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>

<p>The weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$ are unique to each $i$-th head, like fingerprints ğŸ‘†ğŸ¼.</p>

<p><img src="/assets/2024/September/multi%20head.png" alt="Fig: Multi-Head Attention consists of several attention layers running in parallel. [[Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf)]" /></p>

<p><em>Fig: Multi-Head Attention consists of several attention layers running in parallel. [<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. (2017)</a>]</em></p>

<p>ğŸ”‘ Q, ğŸ—ï¸ K, and ğŸ’ V are the query, key, and value matrices, respectively. The number of attention heads is represented by h ğŸ§ . The final output of the multi-head attention operation is obtained by concatenating the output of each attention head and multiplying it by a weight matrix ğŸ§® Wâ°.</p>

<h2 id="scaled-dot-product-attention-">Scaled Dot-Product Attention ğŸ”</h2>

<p>The scaled dot-product attention operation can be defined as: ğŸ§®</p>

<p>$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$</p>

<p>Here, ğŸ”‘ Q, ğŸ—ï¸ K, and ğŸ’ V are the query, key, and value matrices, respectively, and ğŸ“ d_k is the dimension of the key vectors.</p>

<p>Self-attention ğŸ§ ğŸ‘€ is a type of scaled dot-product attention. In self-attention, the query, key, and value vectors are all derived from the same sequence, allowing the model to focus on different parts of the sequence at different times. ğŸ”ğŸ”„</p>

<p>For example, in the case of an encoder, the keys ğŸ—ï¸, values ğŸ’, and queries ğŸ”‘ come from the output of the previous layer of the encoder. ğŸ”ğŸ§±</p>

<h3 id="coding-view">Coding View</h3>

<p>Here is an example of a Transformer architecture implemented in Python using PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">output_seq</span><span class="p">):</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">input_seq</span>

        <span class="k">for</span> <span class="n">enc_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">enc_layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>

        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">output_seq</span>

        <span class="k">for</span> <span class="n">dec_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">dec_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div>

<p>The self-attention mechanism ğŸ§ ğŸ‘€ is used in the <code class="language-plaintext highlighter-rouge">EncoderLayer</code> ğŸ”’ and <code class="language-plaintext highlighter-rouge">DecoderLayer</code> ğŸ”“ classes, which are used to define the encoder and decoder layers of the Transformer architecture, respectively.</p>

<p>Here are the <code class="language-plaintext highlighter-rouge">EncoderLayer</code> ğŸ”’ and <code class="language-plaintext highlighter-rouge">DecoderLayer</code> ğŸ”“ classes used to define the encoder and decoder layers of the Transformer architecture, respectively: ğŸ—ï¸</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">multi_head_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">):</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">norm_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">input_seq</span> <span class="o">+</span> <span class="n">attention_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>
        <span class="n">output_seq</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">norm_output</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_seq</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">source_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output_seq</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="n">self_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">output_seq</span><span class="p">)</span>
        <span class="n">self_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self_attention_output</span><span class="p">)</span>
        <span class="n">norm_output1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">output_seq</span> <span class="o">+</span> <span class="n">self_attention_output</span><span class="p">)</span>
        <span class="n">source_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">source_attention</span><span class="p">(</span><span class="n">norm_output1</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="n">source_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">source_attention_output</span><span class="p">)</span>
        <span class="n">norm_output2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">norm_output1</span> <span class="o">+</span> <span class="n">source_attention_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm_output2</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>
        <span class="n">output_seq</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">norm_output2</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_seq</span>

</code></pre></div></div>

<h3 id="advantages-and-disadvantages-">Advantages and Disadvantages ğŸŒŸğŸš«</h3>

<p>One advantage of the Transformer is its ability to process long sequences of text efficiently ğŸš€. This makes it well-suited for tasks such as machine translation ğŸŒ and language modeling ğŸ“š.</p>

<p>One disadvantage of the Transformer is its high computational cost ğŸ’»ğŸ’°, which can make it difficult to train on large datasets ğŸ“Š.</p>

<h3 id="applications-ï¸">Applications ğŸ› ï¸</h3>

<p>The Transformer has achieved state-of-the-art results in several natural language processing tasks, such as machine translation ğŸŒ, text classification ğŸ“‹, and language modeling ğŸ“. It has also been used in various applications, such as speech recognition ğŸ™ï¸, text generation âœï¸, and image captioning ğŸ–¼ï¸.</p>

<h3 id="useful-resources-">Useful Resources ğŸ“š</h3>

<p>If youâ€™re interested in learning more about the Transformer, here are some useful resources to get you started:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> ğŸ“„ - The original paper introducing the Transformer architecture.</li>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ğŸ¨ - A visual guide to the Transformer architecture.</li>
  <li><a href="http://peterbloem.nl/blog/transformers">Transformers from Scratch</a> ğŸ”¨ - A step-by-step guide to implementing the Transformer from scratch.</li>
</ul>

<p>I hope you found this blog helpful in understanding the Transformer and its applications in natural language processing ğŸ§ ğŸ’¬. If you have any questions or comments, feel free to leave them below! ğŸ’¡ğŸ’¬</p>

  </div><a class="u-url" href="/ai/2023/06/23/Transformer_of_ai_world.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
