<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformer of AI World | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Transformer of AI World" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction 🚀" />
<meta property="og:description" content="Introduction 🚀" />
<link rel="canonical" href="http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html" />
<meta property="og:url" content="http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-06-23T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transformer of AI World" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-06-23T10:00:10+05:30","datePublished":"2023-06-23T10:00:10+05:30","description":"Introduction 🚀","headline":"Transformer of AI World","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html"},"url":"http://localhost:4000/ai/2023/06/23/Transformer_of_ai_world.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformer of AI World</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-06-23T10:00:10+05:30" itemprop="datePublished">Jun 23, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction-">Introduction 🚀</h2>

<p>Are you interested in natural language processing? 🤔 If so, you might have heard of the Transformer, a neural network architecture that has achieved state-of-the-art results in several NLP tasks 🏆, such as machine translation 🌍, text classification 📊, and language modeling 📚. In this blog, we’ll explore what the Transformer is, how it works, its advantages and disadvantages, and some of its applications. 🧐</p>

<h2 id="definition-">Definition 📖</h2>

<p>The Transformer is a type of neural network architecture that relies heavily on attention mechanisms 👀. It was introduced in 2017 by Vaswani et al. and has since become one of the most popular neural network architectures in the field of natural language processing. 🌟</p>

<p>Its success is largely due to its ability to leverage attention mechanisms, which enable the model to focus on specific parts of the input sequence 🔍. This allows the Transformer to process long sequences of text more efficiently than traditional recurrent neural networks. 🚀</p>

<h1 id="working-️">Working ⚙️</h1>

<p>The Transformer architecture is a powerful deep learning model used for natural language processing. It is composed of two main parts: an encoder 🔒 and a decoder 🔓.</p>

<p>The encoder, which is the first part of the model, processes the input sequence by encoding its information into a set of vectors 📊. These vectors capture the meaning of the input sequence and are then passed onto the decoder. 🔄</p>

<p>The decoder is responsible for generating the output sequence, such as a translation or a summary 📝. It does this by decoding the vectors produced by the encoder and generating a new sequence of words based on the encoded information. 🎨</p>

<p><img src="/assets/2024/September/transformers.png" alt="Fig: Transformer - model architecture [[Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf)]" /></p>

<p><em>Fig: Transformer - model architecture [<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. (2017)</a>]</em></p>

<h2 id="encoder-">Encoder 🔒</h2>

<p>The Encoder is made up of N=6 identical layers, each of which has 2 sub-layers. 🧱🔄</p>

<ol>
  <li><strong>Multi-Head Self-Attention Mechanism 🧠👀</strong>: This mechanism calculates multiple sets of attention weights using self-attention, with each set focusing on a different part of the input sequence.</li>
  <li>
    <p><strong>Feed Forward Network 🔄➡️</strong>: This is a fully connected feed-forward network that is applied to each position separately and identically. For this, two linear transformations are used with ReLU activation between them:</p>

    <p>$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$</p>

    <p>Here, $x$ is the input vector, $W_1$ and $W_2$ are weight matrices, $b_1$ and $b_2$ are bias vectors, and $\max(0, x)$ is the ReLU activation function. 🧮📊</p>
  </li>
</ol>

<h2 id="decoder-">Decoder 🔓</h2>

<p>The decoder is also composed of N=6 identical layers. However, the decoder adds an extra layer: 🔓🔢</p>

<ol>
  <li><strong>Multi-Head Attention 🎭👥</strong>: This layer does not use self-attention, but instead calculates attention weights over the output of the encoder stack. Its keys and values come from the output of the encoder layer, while its queries come from the previous decoder layer.</li>
</ol>

<p>After each sub-layer, normalization is performed using layer normalization 📏. This process helps maintain stable activations throughout the network. 🧠</p>

<h4 id="normalization">Normalization</h4>

<p>Normalization involves scaling the input features to have zero mean and unit variance, which helps reduce the effect of differences in the scale of the input features. 📊🔍</p>

<p>Layer normalization is defined as: 🧮</p>

<p>$LayerNorm(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$</p>

<p>where $x$ is the input vector, $\mu$ and $\sigma$ are the mean and standard deviation of $x$, respectively, $\epsilon$ is a small constant to prevent division by zero, $\odot$ is element-wise multiplication, and $\gamma$ and $\beta$ are learnable scale and shift parameters, respectively. 🔬</p>

<p>In addition, positional encoding is added to the input embeddings of the encoder and decoder stacks. 🎯🔢</p>

<h4 id="positional-encoding">Positional encoding</h4>

<p>The positional encoding is defined as: 🧘🏽</p>

<p>$PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})$</p>

<p>$PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p>

<p>where $pos$ is the position of the token in the sequence, $i$ is the dimension of the embedding vector, and $d_{model}$ is the total number of dimensions. 📐</p>

<p>These layers allow the model to learn complex patterns and relationships within the input and output sequences. 🕸️ The self-attention layers help the model focus on the most important parts of the input sequence 🔍, while the feed-forward layers help it make predictions based on the encoded information. 🎯</p>

<p>Think in context of training, both input and output are provided. During backpropagation, the weight matrices of the scaled dot-product attention are updated, as well as the parameters of the feedforward neural layers. This process aims to improve the accuracy of the next output and make the attention weights more reasonable. 🔄📈</p>

<p>Overall, the Transformer architecture is a highly effective model for a wide range of natural language processing tasks, including machine translation 🌍 and text summarization 📝. By using multiple layers of self-attention and feed-forward neural networks, it is able to capture complex relationships and patterns within the input and output sequences, making it a powerful tool for language understanding and generation. 🚀🗣️</p>

<p>Let’s dive into the attention mechanism used in transformers! 🤿🧠</p>

<h2 id="multi-head-attention-">Multi-Head Attention 🎭🔍</h2>

<p>The multi-head attention operation is like a symphony of smaller attention operations 🎻🎺. These mini-operations work together in parallel, creating a harmonious final output 🎵.</p>

<p>Here’s the magical equation for multi-head attention 🧙‍♂️:</p>

<p>$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, …, \text{head}_h)W^O$</p>

<p>Each smaller operation is called an attention “head” 👤. The output of the $i$-th head is calculated like this 🧮:</p>

<p>$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>

<p>The weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$ are unique to each $i$-th head, like fingerprints 👆🏼.</p>

<p><img src="/assets/2024/September/multi%20head.png" alt="Fig: Multi-Head Attention consists of several attention layers running in parallel. [[Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf)]" /></p>

<p><em>Fig: Multi-Head Attention consists of several attention layers running in parallel. [<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. (2017)</a>]</em></p>

<p>🔑 Q, 🗝️ K, and 💎 V are the query, key, and value matrices, respectively. The number of attention heads is represented by h 🧠. The final output of the multi-head attention operation is obtained by concatenating the output of each attention head and multiplying it by a weight matrix 🧮 W⁰.</p>

<h2 id="scaled-dot-product-attention-">Scaled Dot-Product Attention 🔍</h2>

<p>The scaled dot-product attention operation can be defined as: 🧮</p>

<p>$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$</p>

<p>Here, 🔑 Q, 🗝️ K, and 💎 V are the query, key, and value matrices, respectively, and 📏 d_k is the dimension of the key vectors.</p>

<p>Self-attention 🧠👀 is a type of scaled dot-product attention. In self-attention, the query, key, and value vectors are all derived from the same sequence, allowing the model to focus on different parts of the sequence at different times. 🔍🔄</p>

<p>For example, in the case of an encoder, the keys 🗝️, values 💎, and queries 🔑 come from the output of the previous layer of the encoder. 🔁🧱</p>

<h3 id="coding-view">Coding View</h3>

<p>Here is an example of a Transformer architecture implemented in Python using PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">output_seq</span><span class="p">):</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">input_seq</span>

        <span class="k">for</span> <span class="n">enc_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">enc_layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>

        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">output_seq</span>

        <span class="k">for</span> <span class="n">dec_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">dec_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div>

<p>The self-attention mechanism 🧠👀 is used in the <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 🔒 and <code class="language-plaintext highlighter-rouge">DecoderLayer</code> 🔓 classes, which are used to define the encoder and decoder layers of the Transformer architecture, respectively.</p>

<p>Here are the <code class="language-plaintext highlighter-rouge">EncoderLayer</code> 🔒 and <code class="language-plaintext highlighter-rouge">DecoderLayer</code> 🔓 classes used to define the encoder and decoder layers of the Transformer architecture, respectively: 🏗️</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">multi_head_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">):</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">norm_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">input_seq</span> <span class="o">+</span> <span class="n">attention_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>
        <span class="n">output_seq</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">norm_output</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_seq</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">source_attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output_seq</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="n">self_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">output_seq</span><span class="p">)</span>
        <span class="n">self_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self_attention_output</span><span class="p">)</span>
        <span class="n">norm_output1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">output_seq</span> <span class="o">+</span> <span class="n">self_attention_output</span><span class="p">)</span>
        <span class="n">source_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">source_attention</span><span class="p">(</span><span class="n">norm_output1</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="n">source_attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">source_attention_output</span><span class="p">)</span>
        <span class="n">norm_output2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">norm_output1</span> <span class="o">+</span> <span class="n">source_attention_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm_output2</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>
        <span class="n">output_seq</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">norm_output2</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_seq</span>

</code></pre></div></div>

<h3 id="advantages-and-disadvantages-">Advantages and Disadvantages 🌟🚫</h3>

<p>One advantage of the Transformer is its ability to process long sequences of text efficiently 🚀. This makes it well-suited for tasks such as machine translation 🌍 and language modeling 📚.</p>

<p>One disadvantage of the Transformer is its high computational cost 💻💰, which can make it difficult to train on large datasets 📊.</p>

<h3 id="applications-️">Applications 🛠️</h3>

<p>The Transformer has achieved state-of-the-art results in several natural language processing tasks, such as machine translation 🌐, text classification 📋, and language modeling 📝. It has also been used in various applications, such as speech recognition 🎙️, text generation ✍️, and image captioning 🖼️.</p>

<h3 id="useful-resources-">Useful Resources 📚</h3>

<p>If you’re interested in learning more about the Transformer, here are some useful resources to get you started:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 📄 - The original paper introducing the Transformer architecture.</li>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> 🎨 - A visual guide to the Transformer architecture.</li>
  <li><a href="http://peterbloem.nl/blog/transformers">Transformers from Scratch</a> 🔨 - A step-by-step guide to implementing the Transformer from scratch.</li>
</ul>

<p>I hope you found this blog helpful in understanding the Transformer and its applications in natural language processing 🧠💬. If you have any questions or comments, feel free to leave them below! 💡💬</p>

  </div><a class="u-url" href="/ai/2023/06/23/Transformer_of_ai_world.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
