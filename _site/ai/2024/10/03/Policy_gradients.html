<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Policy Gradients | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Policy Gradients" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Policy Gradients are a fundamental concept in reinforcement learning, offering a powerful approach to training agents in complex environments. Letâ€™s explore this fascinating topic in detail! ğŸ§ " />
<meta property="og:description" content="Policy Gradients are a fundamental concept in reinforcement learning, offering a powerful approach to training agents in complex environments. Letâ€™s explore this fascinating topic in detail! ğŸ§ " />
<link rel="canonical" href="http://localhost:4000/ai/2024/10/03/Policy_gradients.html" />
<meta property="og:url" content="http://localhost:4000/ai/2024/10/03/Policy_gradients.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-03T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Policy Gradients" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-03T10:00:10+05:30","datePublished":"2024-10-03T10:00:10+05:30","description":"Policy Gradients are a fundamental concept in reinforcement learning, offering a powerful approach to training agents in complex environments. Letâ€™s explore this fascinating topic in detail! ğŸ§ ","headline":"Policy Gradients","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2024/10/03/Policy_gradients.html"},"url":"http://localhost:4000/ai/2024/10/03/Policy_gradients.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Policy Gradients</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-10-03T10:00:10+05:30" itemprop="datePublished">Oct 3, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Policy Gradients are a fundamental concept in reinforcement learning, offering a powerful approach to training agents in complex environments. Letâ€™s explore this fascinating topic in detail! ğŸ§ </p>

<p>In Reinforcement learning, there are two types of methods value-based methods and policy-based methods.</p>

<h1 id="policy-">Policy ğŸ®</h1>

<p>The algorithm a software agent uses to determine its actions is called its policy. The goal of the agent is to learn a policy that maximizes its reward over time. ğŸ¯</p>

<p>There are several types of policies:</p>

<ol>
  <li><strong>Deterministic policies</strong> ğŸ”’: 
These directly map states to actions</li>
  <li><strong>Stochastic policies</strong> ğŸ²: 
These map states to probability distributions over actions. An action is randomly chosen based on the probability distribution, and itâ€™s not necessary for the agent to always choose the action with the highest probability. This type of policy involves some randomness</li>
</ol>

<h2 id="value-based-methods">Value-Based Methods</h2>

<p>Value-based methods focus on learning the value function and <em>deriving a policy from it</em>. Some popular algorithms include: Q-Learning, SARSA (State-Action-Reward-State-Action), DQN (Deep Q-Network), Double DQN.</p>

<p>Check the other post about Q-learning to learn more about value based methods.</p>

<p><a href="https://ds-meena.github.io/ai/2024/09/23/Q_learning.html">Q-Learning</a></p>

<h2 id="policy-based-methods">Policy-Based Methods</h2>

<p>Policy-based methods refer to any reinforcment learning technicque that <em>directly learn a policy</em> without explicitly computing value functions. These can include:</p>

<ul>
  <li><strong>Policy Iteration</strong>: A method that alternates between policy evaluation and policy improvement.</li>
  <li><strong>Evolutionary Algorithms</strong>: Methods that use principles of biological evolution to optimize policies.</li>
  <li><strong>Policy Gradient Methods</strong>: Techniques that use gradient ascent to optimize the policy directly.</li>
</ul>

<h1 id="policy-gradients-">Policy Gradients ğŸ“ˆ</h1>

<p>In policy gradients, the agent learns to make decisions based on a policy, which is a mapping from states to actions. The method is particularly useful for problems where the environment is fully observable, and the agent can learn through trial and error. ğŸ§ ğŸ”</p>

<p>The policy gradient algorithm is an iterative process that gradually updates the policy parameters to maximize the expected reward. It improves the policy by playing, using gradient ascent and discounted rewards. The optimization is done using gradient ascent methods, such as stochastic gradient ascent. ğŸ”„ğŸ’¹</p>

<p>Policy gradients have been successfully applied to various domains, including robotics and natural language processing. ğŸ¤–ğŸ’¬</p>

<h3 id="the-fundamental-concept-">The Fundamental Concept ğŸ’¡</h3>

<p>The core idea of Policy Gradient methods can be summarized in three steps:</p>

<ol>
  <li>The agent performs an action based on its current policy ğŸ¤–</li>
  <li>The environment provides feedback in the form of a reward signal ğŸ‘€</li>
  <li>The policy is updated to increase the probability of actions that led to high rewards and decrease the probability of actions that led to low rewards ğŸ“ˆğŸ“‰</li>
</ol>

<h2 id="mathematical-framework-">Mathematical Framework ğŸ”¢</h2>

<p>Letâ€™s say we have a simple policy that chooses actions uniformly at random. The expected return for this policy could be calculated as:</p>

\[E_{Ï„âˆ¼Ï€_Î¸}[R(Ï„)] = \frac{1}{N} \sum_{i=1}^N R(Ï„_i)\]

<p>Where N is the number of sampled trajectories, and $R(Ï„_i)$ is the return (sum of rewards) for the $ith$ trajectory.</p>

<p>The Policy Gradient theorem forms the backbone of these methods. The objective is to maximize the expected return:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]\]

<p>Where:</p>

<ul>
  <li>$J(Î¸)$ is the expected return</li>
  <li>$Î¸$ represents the policy parameters</li>
  <li>$Ï„$ is a trajectory (state-action sequence e.g. <code class="language-plaintext highlighter-rouge">Ï„ = [
  (s0, a0, r0, s1),  # (state, action, reward, next state)
  (s1, a1, r1, s2),
  (s2, a2, r2, s3),
  ...
  (sT, aT, rT, sT+1)
])</code></li>
  <li>$Ï€_Î¸$ is the policy</li>
  <li>$R(Ï„)$ is the return of a trajectory</li>
</ul>

<p>The gradient of this objective with respect to the policy parameters is given by:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \lvert s_t) R(\tau) \right]\]

<p>This equation can be interpreted as follows:</p>

<ul>
  <li>$âˆ‡_Î¸ log Ï€_Î¸(a_t\lvert s_t)$ is the gradient of the log probability of taking action $a_t$ in state $s_t$</li>
  <li>$R(Ï„)$ is the return of the trajectory, acting as a weighting factor</li>
  <li>The summation is over all time steps in the trajectory</li>
  <li>The expectation $E$ is taken over all possible trajectories under the current policy</li>
</ul>

<p>This equation tells us how to adjust our policy parameters Î¸ to increase the expected return. In practice, we often estimate this expectation using a finite number of sampled trajectories.</p>

<h2 id="practical-implementation-ï¸">Practical Implementation ğŸ–¥ï¸</h2>

<p>In practice, we often use the following steps to implement Policy Gradient methods:</p>

<ol>
  <li>Initialize the policy parameters $Î¸$ randomly</li>
  <li>For each episode:
    <ol>
      <li>Generate a trajectory $Ï„$ by following the current policy $Ï€_Î¸$</li>
      <li>Calculate the returns $R(Ï„)$ for the trajectory</li>
      <li>
        <p>Update the policy parameters using gradient ascent:</p>

\[Î¸ â† Î¸ + Î± âˆ‡_Î¸ J(Î¸)\]

        <p>Where Î± is the learning rate</p>
      </li>
    </ol>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyGradient</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_states</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_states</span> <span class="o">=</span> <span class="n">n_states</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="n">n_actions</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_build_model</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1"># represents our policy Ï€_Î¸
</span>    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_states</span><span class="p">,)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="k">return</span> <span class="n">model</span>
    
    <span class="c1"># follows stochastic policy Ï€_Î¸(a_t|s_t)
</span>    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_states</span><span class="p">])</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probabilities</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="c1"># optimize the policy given a sequence of states, actions, rewards (trajectory)
</span>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
        <span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
            <span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>

            <span class="c1"># calcualte - log Ï€_Î¸(a_t|s_t) * R(Ï„) = -1 * gain
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="n">rewards</span><span class="p">)</span>
        
        <span class="c1"># calculate gradient of -ve gain
</span>        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        
        <span class="c1"># apply gradient descent
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</code></pre></div></div>

<p>This code implements the Policy gradient method:</p>

<ol>
  <li>The policy $Ï€_Î¸$ is represented by the neural network model.</li>
  <li>The action selection follows the stochastic policy: $Ï€_Î¸(a_t\lvert s_t$).</li>
  <li>
    <p>The train method implements the policy gradient theorem:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t\lvert s_t) R(\tau) \right]\]

    <p>Here, tf.nn.sparse_softmax_cross_entropy_with_logits computes $-log Ï€_Î¸(a_t\lvert s_t)$, and we multiply it by the rewards.</p>
  </li>
  <li>
    <p>The gradient computation and parameter update follow the equation:</p>

\[Î¸ â† Î¸ - Î± (- âˆ‡_Î¸ J(Î¸))\]

    <p>First -ve represents gradient descent and the second -ve represents negative of gradient of gain. That mean it maximizes the rewards.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">Acrobot-v1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">PolicyGradient</span><span class="p">(</span><span class="n">n_states</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_actions</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># trajectory of the episode
</span>    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        
        <span class="n">states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
   
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
    
    <span class="c1"># Normalize rewards
</span>    <span class="n">rewards</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    
    <span class="c1"># Now, optimize the policy using trajectory of the episode
</span>    <span class="n">agent</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</code></pre></div></div>

<p>We have implemented the policy gradient of <a href="https://gymnasium.farama.org/environments/classic_control/acrobot/">Acrobot-v1</a> environment. Where it tries to stand up and avoid penalty of -1.</p>

<h2 id="advantages-and-challenges-">Advantages and Challenges ğŸ†ğŸš§</h2>

<p>Policy Gradient methods offer several benefits:</p>

<ul>
  <li>They can handle continuous action spaces effectively</li>
  <li>They can learn stochastic policies, which can be crucial in certain environments</li>
  <li>They can be more stable than value-based methods in some scenarios</li>
</ul>

<p>However, they also face challenges:</p>

<ul>
  <li>High variance in gradient estimates, which can lead to unstable learning</li>
  <li>Sample inefficiency, often requiring many interactions with the environment</li>
  <li>Sensitivity to hyperparameter choices</li>
</ul>

<h3 id="advanced-variants-">Advanced Variants ğŸš€</h3>

<p>Several advanced algorithms have been developed to address these challenges:</p>

<ul>
  <li><strong>Actor-Critic methods</strong>: Combine policy gradients with value function approximation to reduce variance</li>
  <li><strong>Trust Region Policy Optimization (TRPO)</strong>: Constrains policy updates to improve stability</li>
  <li><strong>Proximal Policy Optimization (PPO)</strong>: A simpler and more efficient version of TRPO</li>
</ul>

<p>Understanding these concepts and their mathematical foundations is crucial for effectively implementing and improving Policy Gradient methods in reinforcement learning applications.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Policy Gradients offer a powerful and flexible approach to reinforcement learning. By directly optimizing the policy, they can handle complex, high-dimensional action spaces and learn stochastic policies. While they face challenges like sample inefficiency and high variance, ongoing research continues to improve these methods, making them an exciting area of study in the field of AI and machine learning. ğŸ“ğŸ”¬</p>

<p>Remember, mastering policy gradients takes practice and experimentation. Happy learning! ğŸš€ğŸ¤–</p>

  </div><a class="u-url" href="/ai/2024/10/03/Policy_gradients.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
