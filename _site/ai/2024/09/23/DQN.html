<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Q-Learning | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Deep Q-Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/ai/2024/09/23/DQN.html" />
<meta property="og:url" content="http://localhost:4000/ai/2024/09/23/DQN.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-23T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Q-Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-09-23T10:00:10+05:30","datePublished":"2024-09-23T10:00:10+05:30","description":"Introduction","headline":"Deep Q-Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2024/09/23/DQN.html"},"url":"http://localhost:4000/ai/2024/09/23/DQN.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Q-Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-09-23T10:00:10+05:30" itemprop="datePublished">Sep 23, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>In this blog, we will learn about the fundamental algorithms used in reinforcement learning. It‚Äôs not about neural networks but the mathematical algorithms involved in learning.</p>

<h1 id="markov-decision-process-">Markov Decision Process ü§î</h1>

<p>Let‚Äôs understand the problem, we are trying to solve here. The environment of an agent can be modelled as a Markov decision process, where the agent can choose one of several actions and the transition probabilities depend on the chosen action. ü§ñ</p>

<p>Our aim is to find an optimal policy for the agent, by following that agent can maximize the rewards earned in the enviornment.</p>

<p><img src="/assets/2024/September/markov%20decision%20chain.png" alt="alt text" />
<em>Fig: Example of Markov chain  <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">credit for image</a></em></p>

<p>Let‚Äôs learn some of the algorithms that are used to find the optimal policy for the agent.</p>

<h2 id="state-value-iteration-algorithm-">(State) Value Iteration algorithm üîÑ</h2>

<p>In this algorithm, we calcualte the state value $V(s)$ for all states.</p>

<p>Optimal state value $V^*(s)$ of any state s, is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally. üéØ</p>

<p>$V\star(s) = max_a \sum_sP(s,a,s\prime)[R(s,a,s\prime)+\gamma.V^*(s\prime)]$  for all s</p>

<p><em>Eq: Bellman Optimality Equation</em></p>

<p>where,</p>

<ul>
  <li>$P(s,a,s‚Äô)$ = transition probability from state s to state s‚Äô, given that agent chose action a [conditional probability]. üé≤</li>
  <li>$R(s,a,s‚Äô)$ = reward the agent gets when it goes from state s to state s‚Äô, given that agent chose action a üèÜ</li>
  <li>$\gamma$ = discount factor üàπ</li>
</ul>

<p>If we increase discount factor, we will value the future rewards more.
Bellman optimality equation assumes, that we already have the optimal state value for next state s‚Äô. Since, we don‚Äôt have future value; we update state values iteratively as follows:</p>

<ol>
  <li>First initialize all the state value estimates to 0.</li>
  <li>
    <p>Iteratively update them using recurrent relation</p>

    <p>$V_{k+1}(s) \leftarrow  \underset{a}{\max} \underset{s‚Äô}{\sum}P(s,a,s‚Äô) [R(s,a,s‚Äô) + \gamma.V_k(s‚Äô)]$ for all s</p>

    <p><em>Eq: Value Iteration algorithm</em> üîÅ</p>

    <p>where</p>

    <ul>
      <li>$V_k(s)$ = estimated value of state s at the $k^{th}$ iteration</li>
    </ul>
  </li>
</ol>

<p>After the Value Iteration algorithm converges, we can derive the optimal policy $œÄ^\star$ for each state s: ü•≥</p>

\[\pi^*(s) = \underset{a}{argmax} \sum_{s'} P(s, a, s')[R(s,a,s') + \gamma V^*(s')]\]

<p>This means that for each state, the optimal action is the one that maximizes the expected sum of the immediate reward and the discounted optimal value of the next state. üí∞</p>

<h2 id="q-value-iteration-algorithm-">Q-Value Iteration algorithm üé≤</h2>

<p>This algorithm is used to find the optimal state-action values, genreally called Q-values (Quality values). üí°</p>

<p>Optimal Q-value of state-action pair (s, a), $Q^*(s, a)$, is the sum of discounted future rewards the agent can expect on average after it reaches state s and chooses an action a. üí∞</p>

<p>It involves following steps:</p>
<ol>
  <li>Initialize all Q-values estimates to 0.</li>
  <li>
    <p>Then update them using below recurrence relation. üîÑ</p>

    <p>$Q_{k+1}(s,a) \leftarrow \underset{s‚Äô}{\sum}T(s,a,s‚Äô)[R(s,a,s‚Äô)+\gamma.\underset{a‚Äô}{max} \space Q_k(s‚Äô,a‚Äô)]$</p>

    <p><em>Eq: Q-Value Iteration algorithm</em></p>

    <p>where:</p>
    <ul>
      <li>$\underset{a‚Äô}{max} \space Q_k(s‚Äô, a‚Äô)$ is the maximum Q-value for the next state s‚Äô and all possible actions a‚Äô at $k_{th}$ iteratin</li>
    </ul>
  </li>
</ol>

<p>After the Q-Value Iteration algorithm converges, we can derive the optimal policy $\pi^*(s)$ for each state s.</p>

\[\pi^*(s) = \underset{a}{argmax} \space Q^\star(s,a)\]

<p>That means, when the agent is in state s it should choose the action with the highest Q-Value for that state. üèÜ</p>

<p>Let‚Äôs apply the Q-Value Iteration algorithm to MDP given in above image:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># shape=[s, a, s']  # row - current state, column = action
# s2 to s0 given action a1 transition probability = [2][1][0]
</span><span class="n">transition_probabilities</span> <span class="o">=</span> <span class="p">[</span> 
		<span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]],</span> 
		<span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="bp">None</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span> 
		<span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span>
	<span class="p">]</span>

<span class="c1"># shape=[s, a, s']
</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span>  
		<span class="p">[[</span><span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> 
		<span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">]],</span> 
		<span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">+</span><span class="mi">40</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
	<span class="p">]</span>

<span class="c1"># from s0, s1, s2
</span><span class="n">possible_actions</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>   

<span class="c1"># Initialize Q-Values
</span><span class="n">Q_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># -np.inf for impossible actions
</span><span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">):</span>
	<span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">actions</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>     <span class="c1"># 0 for possible actions
</span>	
<span class="c1"># Q-Value Iteration algorithm
</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.90</span>

<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
	<span class="n">Q_prev</span> <span class="o">=</span> <span class="n">Q_values</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
	
	<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">possible_actions</span><span class="p">[</span><span class="n">s</span><span class="p">]:</span>

			<span class="n">Q_values</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">transition_probabilities</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">][</span><span class="n">sp</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">][</span><span class="n">sp</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">Q_prev</span><span class="p">[</span><span class="n">sp</span><span class="p">]))</span>
				<span class="k">for</span> <span class="n">sp</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
                        
<span class="nf">print</span><span class="p">(</span><span class="n">Q_values</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best action for each state: </span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">Q_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># [[18.91891892 17.02702702 13.62162162]
#  [ 0.                -inf -4.87971488]
#  [       -inf 50.13365013        -inf]]
# Best action for each state  [0 0 1]
</span></code></pre></div></div>

<p>Using above algorithm we can find the best policy for the agent.</p>

<h1 id="q-learning-">Q-Learning ü§ñ</h1>

<p>If you notice in the above MDP diagram, the transition probabilities and rewards are given us in advance. That‚Äôs not the case in real word, now comes the role of Q-Learning algorithm. <strong>Q-Learning algorithm</strong> is an adaptation of the Q-Value Iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown.</p>

<p>This algorithm is useful for problems where the environment is fully observable, and the agent can learn by trial and error. Q-learning has been successfully applied to problems such as game playing, robotics, and natural language processing. üß†ü§ñ</p>

<p>$Q(s,a) \underset {\alpha}{\leftarrow} r + \gamma.\underset{a‚Äô}{max} \space Q(s‚Äô, a‚Äô)$</p>

<p><em>Eq: Q-Learning algorithm</em></p>

<p>$ old \underset {\alpha}{\leftarrow} new ‚áí old(1-a) + a*new$ [This is how be interpret the above equation]</p>

<h2 id="q-learning-algorithm-">Q-learning algorithm üß†</h2>

<ol>
  <li>Initialize the Q-table with arbitrary values for all state-action pairs.</li>
  <li>Observe the current state.</li>
  <li>Select an action to take based on the current state and the values in the Q-table. This can be done using an exploration-exploitation strategy such as epsilon-greedy.</li>
  <li>Take the selected action and observe the reward and the new state. (a, r, s‚Äô)</li>
  <li>
    <p>Update the Q-value for the state-action pair that was just taken based on the observed reward and the maximum Q-value for the new state.</p>

    <p>The Q-learning algorithm uses the following equation to update the Q-value for a state-action pair:</p>

    <p>$Q(s,a) {\leftarrow} (1-\alpha)Q(s,a) + \alpha(  r + \gamma.\underset{a‚Äô}{max} \space Q(s‚Äô, a‚Äô))$</p>

    <p>Where:</p>

    <ul>
      <li>Q(s, a) is the Q-value for state s and action a</li>
      <li>Œ± is the learning rate, which determines how much the Q-value is updated in each iteration</li>
      <li>r is the reward received for taking action a in state s</li>
      <li>Œ≥ is the discount factor, which determines the importance of future rewards</li>
      <li>$\underset{a‚Äô}{max} \space Q(s‚Äô, a‚Äô)$ is the maximum Q-value for the next state s‚Äô and all possible actions a‚Äô (maximum future reward estimate)</li>
      <li>s‚Äô is the next state reached after taking action a in state s</li>
    </ul>
  </li>
  <li>Repeat üîÑ steps 2-5 until the algorithm converges or a maximum number of iterations is reached.</li>
</ol>

<p>The optimal policy üèÜ can be derived by selecting the action with the highest Q-value for each state as in Q-value Iteration algorithm.</p>

<p>Let‚Äôs implement Q-Learning algorithm using open AI gym environment (Taxi-v3). üöï</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">Taxi-v3</span><span class="sh">'</span><span class="p">)</span>

<span class="n">Q_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">])</span>

<span class="c1"># exploration policy
</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Exploration rate
</span>
<span class="k">def</span> <span class="nf">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>  <span class="c1"># Explore
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>  <span class="c1"># Exploit
</span></code></pre></div></div>

<p>Q-Learning algorithm with learning rate decay: ‚ò¢Ô∏è</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Hyperparameters
</span><span class="n">alpha0</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Initial learning rate
</span><span class="n">decay</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># Discount factor
</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        
        <span class="c1"># Q-learning update
</span>        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">episode</span> <span class="o">*</span> <span class="n">decay</span><span class="p">)</span>
        
        <span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>
        <span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
        
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</code></pre></div></div>

<p><img src="/assets/2024/September/Q_learning.png" alt="Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don‚Äôt know anything) (right)" /></p>

<p><em>Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don‚Äôt know anything) (right) <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">credit for image</a></em></p>

<p>Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!</p>

<h3 id="advantage">Advantage</h3>

<p>It can learn optimal policies without requiring a model of the environment. (Model-free reinforcement learning algorithm).  Instead, it learns directly from experience by updating the Q-values based on observed rewards and transitions between states.</p>

<h3 id="disadvantage">Disadvantage</h3>

<p>It can be computationally expensive and may require a large amount of data to converge to an optimal solution.</p>

<p>Overall, Q-learning is a powerful technique with many potential applications, but it is important to carefully consider the problem and the available data before choosing a Q-learning approach.</p>

<h2 id="references">References</h2>

<ol>
  <li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Buy here</a></li>
</ol>

  </div><a class="u-url" href="/ai/2024/09/23/DQN.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
