<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Q-Learning | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Deep Q-Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/ai/2024/09/20/DQN.html" />
<meta property="og:url" content="http://localhost:4000/ai/2024/09/20/DQN.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-20T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Q-Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-09-20T10:00:10+05:30","datePublished":"2024-09-20T10:00:10+05:30","description":"Introduction","headline":"Deep Q-Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2024/09/20/DQN.html"},"url":"http://localhost:4000/ai/2024/09/20/DQN.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Q-Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-09-20T10:00:10+05:30" itemprop="datePublished">Sep 20, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>In this blog, we will learn about the fundamental algorithms used in reinforcement learning. Itâ€™s not about neural networks but the mathematical algorithms involved in learning.</p>

<h1 id="markov-decision-process-">Markov Decision Process ğŸ¤”</h1>

<p>Letâ€™s understand the problem, we are trying to solve here. The environment of an agent can be modelled as a Markov decision process, where the agent can choose one of several actions and the transition probabilities depend on the chosen action. ğŸ¤–</p>

<p>Our aim is to find an optimal policy for the agent, by following that agent can maximize the rewards earned in the enviornment.</p>

<p><img src="/assets/2024/September/markov%20decision%20chain.png" alt="alt text" />
<em>Fig: Example of Markov chain</em></p>

<p>Letâ€™s learn some of the algorithms that are used to find the optimal policy for the agent.</p>

<h2 id="state-value-iteration-algorithm-">(State) Value Iteration algorithm ğŸ”„</h2>

<p>In this algorithm, we calcualte the state value $V(s)$ for all states.</p>

<p>Optimal state value $V^*(s)$ of any state s, is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally. ğŸ¯</p>

<p>$V\star(s) = max_a \sum_sP(s,a,s\prime)[R(s,a,s\prime)+\gamma.V^*(s\prime)]$  for all s</p>

<p><em>Eq: Bellman Optimality Equation</em></p>

<p>where,</p>

<ul>
  <li>$P(s,a,sâ€™)$ = transition probability from state s to state sâ€™, given that agent chose action a [conditional probability]. ğŸ²</li>
  <li>$R(s,a,sâ€™)$ = reward the agent gets when it goes from state s to state sâ€™, given that agent chose action a ğŸ†</li>
  <li>$\gamma$ = discount factor ğŸˆ¹</li>
</ul>

<p>Bellman optimality equation assumes, that we already have the optimal state value for next state sâ€™. Since, we donâ€™t have future value; we update state values iteratively as follows:</p>

<ol>
  <li>First initialize all the state value estimates to 0.</li>
  <li>
    <p>Iteratively update them using recurrent relation</p>

    <p>$V_{k+1}(s) \leftarrow  \underset{a}{\max} \underset{sâ€™}{\sum}P(s,a,sâ€™) [R(s,a,sâ€™) + \gamma.V_k(sâ€™)]$ for all s</p>

    <p><em>Eq: Value Iteration algorithm</em> ğŸ”</p>

    <p>where</p>

    <ul>
      <li>$V_k(s)$ = estimated value of state s at the $k^{th}$ iteration</li>
    </ul>
  </li>
</ol>

<p>After the Value Iteration algorithm converges, we can derive the optimal policy $Ï€^\star$ for each state s: ğŸ¥³</p>

\[\pi^*(s) = \underset{a}{argmax} \sum_{s'} P(s, a, s')[R(s,a,s') + \gamma V^*(s')]\]

<p>This means that for each state, the optimal action is the one that maximizes the expected sum of the immediate reward and the discounted optimal value of the next state. ğŸ’°</p>

<h2 id="q-value-iteration-algorithm-">Q-Value Iteration algorithm ğŸ²</h2>

<p>This algorithm is used to find the optimal state-action values, genreally called Q-values (Quality values). ğŸ’¡</p>

<p>Optimal Q-value of state-action pair (s, a), $Q^*(s, a)$, is the sum of discounted future rewards the agent can expect on average after it reaches state s and chooses an action a. ğŸ’°</p>

<p>It involves following steps:</p>
<ol>
  <li>Initialize all Q-values estimates to 0.</li>
  <li>
    <p>Then update them using below recurrence relation. ğŸ”„</p>

    <p>$Q_{k+1}(s,a) \leftarrow \underset{sâ€™}{\sum}T(s,a,sâ€™)[R(s,a,sâ€™)+\gamma.\underset{aâ€™}{max} \space Q_k(sâ€™,aâ€™)]$</p>

    <p><em>Eq: Q-Value Iteration algorithm</em></p>
  </li>
</ol>

<p>After the Q-Value Iteration algorithm converges, we can derive the optimal policy $\pi^*(s)$ for each state s.</p>

\[\pi^*(s) = \underset{a}{argmax} \space Q^\star(s,a)\]

<p>That means, when the agent is in state s it should choose the action with the highest Q-Value for that state. ğŸ†</p>

<h1 id="q-learning-">Q-Learning ğŸ¦†</h1>

<p>Q-Learning algorithm is an adaptation of the <strong>Q-Value Iteration</strong> algorithm to the situation where the transition probabilities and the rewards are initially unknown.</p>

<p>This algorithm is useful for problems where the environment is fully observable, and the agent can learn by trial and error. Q-learning has been successfully applied to problems such as game playing, robotics, and natural language processing.</p>

<p>$Q(s,a) \underset {\alpha}{\leftarrow} r + \gamma.\underset{aâ€™}{max} \space Q(sâ€™, aâ€™)$</p>

<p>Eq: Q-Learning algorithm</p>

<p>old â† new â‡’ old(1-a) + a*new [This is how be interpret the above equation]</p>

<h2 id="q-learning-algorithm">Q-learning algorithm</h2>

<ol>
  <li>Initialize the Q-table with arbitrary values for all state-action pairs.</li>
  <li>Observe the current state.</li>
  <li>Select an action to take based on the current state and the values in the Q-table. This can be done using an exploration-exploitation strategy such as epsilon-greedy.</li>
  <li>Take the selected action and observe the reward and the new state. (a, r, sâ€™)</li>
  <li>
    <p>Update the Q-value for the state-action pair that was just taken based on the observed reward and the maximum Q-value for the new state.</p>

    <p>The Q-learning algorithm uses the following equation to update the Q-value for a state-action pair:</p>

    <p>$Q(s,a) {\leftarrow} (1-\alpha)Q(s,a) + \alpha(  r + \gamma.\underset{aâ€™}{max} \space Q(sâ€™, aâ€™))$</p>

    <p>Where:</p>

    <ul>
      <li>Q(s, a) is the Q-value for state s and action a</li>
      <li>Î± is the learning rate, which determines how much the Q-value is updated in each iteration</li>
      <li>r is the reward received for taking action a in state s</li>
      <li>Î³ is the discount factor, which determines the importance of future rewards</li>
      <li>max Q(sâ€™, aâ€™) is the maximum Q-value for the next state sâ€™ and all possible actions aâ€™ (maximum future reward estimate)</li>
      <li>sâ€™ is the next state reached after taking action a in state s</li>
    </ul>
  </li>
  <li>Repeat steps 2-5 until the algorithm converges or a maximum number of iterations is reached.</li>
</ol>

<p>The optimal policy can be derived by selecting the action with the highest Q-value for each state.</p>

<p>Letâ€™s implement Q-Learning algorithm.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># step function for agent
</span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
	<span class="n">probas</span> <span class="o">=</span> <span class="n">transition_probabilities</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
	<span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">probas</span><span class="p">)</span>
	<span class="n">reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="n">next_state</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>

<span class="k">def</span> <span class="nf">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
</code></pre></div></div>

<p>Q-Learning algorithm with learning rate decay:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha0</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># initial learning rate
</span><span class="n">decay</span> <span class="o">=</span> <span class="mf">0.005</span> 
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.90</span> <span class="c1"># discount factor
</span><span class="n">state</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial state
</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
	<span class="n">action</span> <span class="o">=</span> <span class="nf">exploration_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
	<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
	<span class="n">next_value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
	<span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">iteration</span> <span class="o">*</span> <span class="n">decay</span><span class="p">)</span>   <span class="c1"># i think it should be alpha
</span>	
	<span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>   <span class="c1"># old
</span>	<span class="n">Q_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_value</span><span class="p">)</span>  <span class="c1"># new
</span>	
	<span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</code></pre></div></div>

<p><img src="/assets/2024/September/Q_learning.png" alt="Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (donâ€™t know anything) (right)" /></p>

<p>Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (donâ€™t know anything) (right)</p>

<p>Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!</p>

<p><strong>Advantage:</strong></p>

<p>It can learn optimal policies without requiring a model of the environment. (Model-free reinforcement learning algorithm).  Instead, it learns directly from experience by updating the Q-values based on observed rewards and transitions between states.</p>

<p><strong>Disadvantage:</strong></p>

<p>It can be computationally expensive and may require a large amount of data to converge to an optimal solution.</p>

<p>Overall, Q-learning is a powerful technique with many potential applications, but it is important to carefully consider the problem and the available data before choosing a Q-learning approach.</p>

<h2 id="references">References</h2>

<ol>
  <li>Hands on Machine learning with keras and Tensorflow</li>
</ol>

  </div><a class="u-url" href="/ai/2024/09/20/DQN.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
