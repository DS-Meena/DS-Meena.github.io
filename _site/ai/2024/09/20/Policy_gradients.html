<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Policy Gradients | dsm Blogs</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Policy Gradients" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this blog, we will discuss about the policy and policy gradients in reinforcement learning." />
<meta property="og:description" content="In this blog, we will discuss about the policy and policy gradients in reinforcement learning." />
<link rel="canonical" href="http://localhost:4000/ai/2024/09/20/Policy_gradients.html" />
<meta property="og:url" content="http://localhost:4000/ai/2024/09/20/Policy_gradients.html" />
<meta property="og:site_name" content="dsm Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-20T10:00:10+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Policy Gradients" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-09-20T10:00:10+05:30","datePublished":"2024-09-20T10:00:10+05:30","description":"In this blog, we will discuss about the policy and policy gradients in reinforcement learning.","headline":"Policy Gradients","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ai/2024/09/20/Policy_gradients.html"},"url":"http://localhost:4000/ai/2024/09/20/Policy_gradients.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="dsm Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">dsm Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--Added Math Latext support-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Policy Gradients</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-09-20T10:00:10+05:30" itemprop="datePublished">Sep 20, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this blog, we will discuss about the policy and policy gradients in reinforcement learning.</p>

<h1 id="policy-">Policy ðŸŽ®</h1>

<p>The algorithm a software agent uses to determine itâ€™s actions is called itâ€™s policy. The goal of the agent is to learn a policy that maximizes its reward over time. ðŸŽ¯</p>

<p>There are several types of policies:</p>

<ol>
  <li>Deterministic policies ðŸ”’: 
These directly map states to actions</li>
  <li>Stochastic policies ðŸŽ²: 
These map states to probability distributions over actions. An action is randomly chosen based on the probability distribution, and itâ€™s not necessary for the agent to always choose the action with the highest probability. This type of policy involves some randomness.</li>
</ol>

<p>This distinction between deterministic and stochastic policies is an important concept in reinforcement learning and policy-based methods. ðŸ§ </p>

<h1 id="policy-gradients">Policy Gradients</h1>

<p>Policy gradients is a type of reinforcement learning algorithm where the agent learns to make decisions based on a policy, which is a mapping from states to actions. [In naive terms, improves its policy by playing, using gradient descent  &amp; discounted rewards]</p>

<p>This method is useful for problems where the environment is fully observable, and the agent can learn by trial and error. Policy gradients have been successfully applied to problems such as robotics and natural language processing.</p>

<p>The policy gradient algorithm is an iterative process that gradually updates the policy parameters to maximize the expected reward. The optimization is done using gradient descent methods, such as stochastic gradient descent.</p>

<h2 id="algorithm">Algorithm</h2>

<p>One popular class of PG algorithms called REINFORCE algorithms (1992). Common variant:</p>

<ol>
  <li>Let the neural network play the game several times and keep calculating the gradients that would make the chosen action more likely - but donâ€™t apply yet.</li>
  <li>Once you have run several episodes, compute each actionâ€™s advantage (+ve for good, -ve for bad).</li>
  <li>Multiply each gradient vector by the corresponding actionâ€™s advantage.</li>
  <li>Finally compute the mean of all resulting gradient vectors, and use it to perform a Gradient Descent step.</li>
</ol>

  </div><a class="u-url" href="/ai/2024/09/20/Policy_gradients.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">dsm Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">dsm Blogs</li><li><a class="u-email" href="mailto:dharamsinghmeena2000@gmail.com">dharamsinghmeena2000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/DS-Meena"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">DS-Meena</span></a></li><li><a href="https://www.twitter.com/DSMOfficial1"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">DSMOfficial1</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is a blog about Data Science and Machine Learning. I write about all the things I learn in this domain. I also share my knowledge with you.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
