---
layout: post
title:  "Deep Q-Learning"
date:   2024-09-20 10:00:10 +0530
categories: AI
---

# Q-Learning

Q-Learning algorithm is an adaptation of the Q-Value Iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown.

This algorithm is useful for problems where the environment is fully observable, and the agent can learn by trial and error. Q-learning has been successfully applied to problems such as game playing, robotics, and natural language processing.

$Q(s,a) \underset {\alpha}{\leftarrow} r + \gamma.\underset{a'}{max} \space Q(s', a')$

Eq: Q-Learning algorithm

old ← new ⇒ old(1-a) + a*new [This is how be interpret the above equation]

## Q-learning algorithm

1. Initialize the Q-table with arbitrary values for all state-action pairs.
2. Observe the current state.
3. Select an action to take based on the current state and the values in the Q-table. This can be done using an exploration-exploitation strategy such as epsilon-greedy.
4. Take the selected action and observe the reward and the new state. (a, r, s’)
5. Update the Q-value for the state-action pair that was just taken based on the observed reward and the maximum Q-value for the new state.
    
    The Q-learning algorithm uses the following equation to update the Q-value for a state-action pair:
    
    $Q(s,a) {\leftarrow} (1-\alpha)Q(s,a) + \alpha(  r + \gamma.\underset{a'}{max} \space Q(s', a'))$
    
    Where:
    
    - Q(s, a) is the Q-value for state s and action a
    - α is the learning rate, which determines how much the Q-value is updated in each iteration
    - r is the reward received for taking action a in state s
    - γ is the discount factor, which determines the importance of future rewards
    - max Q(s', a') is the maximum Q-value for the next state s' and all possible actions a' (maximum future reward estimate)
    - s' is the next state reached after taking action a in state s
    
6. Repeat steps 2-5 until the algorithm converges or a maximum number of iterations is reached.

The optimal policy can be derived by selecting the action with the highest Q-value for each state.

Let’s implement Q-Learning algorithm.

```python
# step function for agent
def step(state, action):
	probas = transition_probabilities[state][action]
	next_state = np.random.choice([0,1,2], p=probas)
	reward = rewards[state][action][next_state]
	return next_state, reward

def exploration_policy(state):
	return np.random.choice(possible_actions[state])
```

Q-Learning algorithm with learning rate decay:

```python
alpha0 = 0.05 # initial learning rate
decay = 0.005 
gamma = 0.90 # discount factor
state = 0 # initial state

for iteration in range(10000):
	action = exploration_policy(state)
	next_state, reward = step(state, action)
	next_value = np.max(Q_values[next_state])
	alpha = alpha0 / (1 + iteration * decay)   # i think it should be alpha
	
	Q_values[state, action] *= 1 - alpha   # old
	Q_values[state, action] += alpha * (reward + gamma * next_value)  # new
	
	state = next_state
```

![Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don’t know anything) (right)](/assets/2024/September/Q_learning.png)

Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don’t know anything) (right)

Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!

**Advantage:**

It can learn optimal policies without requiring a model of the environment. (Model-free reinforcement learning algorithm).  Instead, it learns directly from experience by updating the Q-values based on observed rewards and transitions between states.

**Disadvantage:**

It can be computationally expensive and may require a large amount of data to converge to an optimal solution.

Overall, Q-learning is a powerful technique with many potential applications, but it is important to carefully consider the problem and the available data before choosing a Q-learning approach.