---
layout: post
title:  "Deep Q-Learning"
date:   2024-09-20 10:00:10 +0530
categories: AI
---

# Introduction

In this blog, we will learn about the fundamental algorithms used in reinforcement learning. It's not about neural networks but the mathematical algorithms involved in learning.

# Markov Decision Process ü§î

Let's understand the problem, we are trying to solve here. The environment of an agent can be modelled as a Markov decision process, where the agent can choose one of several actions and the transition probabilities depend on the chosen action. ü§ñ

Our aim is to find an optimal policy for the agent, by following that agent can maximize the rewards earned in the enviornment.

![alt text](/assets/2024/September/markov%20decision%20chain.png)
*Fig: Example of Markov chain*

Let's learn some of the algorithms that are used to find the optimal policy for the agent.

## (State) Value Iteration algorithm üîÑ

In this algorithm, we calcualte the state value $V(s)$ for all states.

Optimal state value $V^*(s)$ of any state s, is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally. üéØ

$V\star(s) = max_a \sum_sP(s,a,s\prime)[R(s,a,s\prime)+\gamma.V^*(s\prime)]$  for all s

*Eq: Bellman Optimality Equation*

where, 

- $P(s,a,s‚Äô)$ = transition probability from state s to state s‚Äô, given that agent chose action a [conditional probability]. üé≤
- $R(s,a,s‚Äô)$ = reward the agent gets when it goes from state s to state s‚Äô, given that agent chose action a üèÜ
- $\gamma$ = discount factor üàπ

Bellman optimality equation assumes, that we already have the optimal state value for next state s'. Since, we don't have future value; we update state values iteratively as follows:

1. First initialize all the state value estimates to 0.
2. Iteratively update them using recurrent relation  

	$V_{k+1}(s) \leftarrow  \underset{a}{\max} \underset{s'}{\sum}P(s,a,s') [R(s,a,s') + \gamma.V_k(s')]$ for all s 

	*Eq: Value Iteration algorithm* üîÅ

	where

	- $V_k(s)$ = estimated value of state s at the $k^{th}$ iteration

After the Value Iteration algorithm converges, we can derive the optimal policy $œÄ^\star$ for each state s: ü•≥

$$
\pi^*(s) = \underset{a}{argmax} \sum_{s'} P(s, a, s')[R(s,a,s') + \gamma V^*(s')]
$$

This means that for each state, the optimal action is the one that maximizes the expected sum of the immediate reward and the discounted optimal value of the next state. üí∞

## Q-Value Iteration algorithm üé≤

This algorithm is used to find the optimal state-action values, genreally called Q-values (Quality values). üí°

Optimal Q-value of state-action pair (s, a), $Q^*(s, a)$, is the sum of discounted future rewards the agent can expect on average after it reaches state s and chooses an action a. üí∞

It involves following steps:
1. Initialize all Q-values estimates to 0.
2. Then update them using below recurrence relation. üîÑ

	$Q_{k+1}(s,a) \leftarrow \underset{s'}{\sum}T(s,a,s')[R(s,a,s')+\gamma.\underset{a'}{max} \space Q_k(s',a')]$

	*Eq: Q-Value Iteration algorithm*

After the Q-Value Iteration algorithm converges, we can derive the optimal policy $\pi^*(s)$ for each state s.

$$\pi^*(s) = \underset{a}{argmax} \space Q^\star(s,a)$$

That means, when the agent is in state s it should choose the action with the highest Q-Value for that state. üèÜ

# Q-Learning ü¶Ü

Q-Learning algorithm is an adaptation of the **Q-Value Iteration** algorithm to the situation where the transition probabilities and the rewards are initially unknown.

This algorithm is useful for problems where the environment is fully observable, and the agent can learn by trial and error. Q-learning has been successfully applied to problems such as game playing, robotics, and natural language processing.

$Q(s,a) \underset {\alpha}{\leftarrow} r + \gamma.\underset{a'}{max} \space Q(s', a')$

Eq: Q-Learning algorithm

old ‚Üê new ‚áí old(1-a) + a*new [This is how be interpret the above equation]

## Q-learning algorithm

1. Initialize the Q-table with arbitrary values for all state-action pairs.
2. Observe the current state.
3. Select an action to take based on the current state and the values in the Q-table. This can be done using an exploration-exploitation strategy such as epsilon-greedy.
4. Take the selected action and observe the reward and the new state. (a, r, s‚Äô)
5. Update the Q-value for the state-action pair that was just taken based on the observed reward and the maximum Q-value for the new state.
    
    The Q-learning algorithm uses the following equation to update the Q-value for a state-action pair:
    
    $Q(s,a) {\leftarrow} (1-\alpha)Q(s,a) + \alpha(  r + \gamma.\underset{a'}{max} \space Q(s', a'))$
    
    Where:
    
    - Q(s, a) is the Q-value for state s and action a
    - Œ± is the learning rate, which determines how much the Q-value is updated in each iteration
    - r is the reward received for taking action a in state s
    - Œ≥ is the discount factor, which determines the importance of future rewards
    - max Q(s', a') is the maximum Q-value for the next state s' and all possible actions a' (maximum future reward estimate)
    - s' is the next state reached after taking action a in state s
    
6. Repeat steps 2-5 until the algorithm converges or a maximum number of iterations is reached.

The optimal policy can be derived by selecting the action with the highest Q-value for each state.

Let‚Äôs implement Q-Learning algorithm.

```python
# step function for agent
def step(state, action):
	probas = transition_probabilities[state][action]
	next_state = np.random.choice([0,1,2], p=probas)
	reward = rewards[state][action][next_state]
	return next_state, reward

def exploration_policy(state):
	return np.random.choice(possible_actions[state])
```

Q-Learning algorithm with learning rate decay:

```python
alpha0 = 0.05 # initial learning rate
decay = 0.005 
gamma = 0.90 # discount factor
state = 0 # initial state

for iteration in range(10000):
	action = exploration_policy(state)
	next_state, reward = step(state, action)
	next_value = np.max(Q_values[next_state])
	alpha = alpha0 / (1 + iteration * decay)   # i think it should be alpha
	
	Q_values[state, action] *= 1 - alpha   # old
	Q_values[state, action] += alpha * (reward + gamma * next_value)  # new
	
	state = next_state
```

![Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don‚Äôt know anything) (right)](/assets/2024/September/Q_learning.png)

Fig: The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm (don‚Äôt know anything) (right)

Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!

**Advantage:**

It can learn optimal policies without requiring a model of the environment. (Model-free reinforcement learning algorithm).  Instead, it learns directly from experience by updating the Q-values based on observed rewards and transitions between states.

**Disadvantage:**

It can be computationally expensive and may require a large amount of data to converge to an optimal solution.

Overall, Q-learning is a powerful technique with many potential applications, but it is important to carefully consider the problem and the available data before choosing a Q-learning approach.

## References

1. Hands on Machine learning with keras and Tensorflow